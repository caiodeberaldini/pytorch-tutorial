{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors, operating it and particularities\n",
    "\n",
    "## Empty tensor\n",
    "# x = torch.empty(1) # 1-D empty tensor\n",
    "# x = torch.empty(2, 1) # 2-D empty tensor\n",
    "# x = torch.empty(n-dimensional tensor) # n-D empty tensor\n",
    "\n",
    "## Zeros tensor\n",
    "# x = torch.zeros(1) # 1-D zeros tensor\n",
    "\n",
    "## Ones tensor\n",
    "# x = torch.ones(1) # 1-D ones tensor\n",
    "\n",
    "## Defining type\n",
    "# x = torch.ones(2,2, dtype=torch.float16)\n",
    "# print(x.dtype)\n",
    "\n",
    "## Retrieving tensor size\n",
    "# print(x.size())\n",
    "\n",
    "## Creating tensor \"from scratch\"\n",
    "# x = torch.tensor([[2.5, 5], [1.7, 8]])\n",
    "\n",
    "## Operations\n",
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "### Arithmetic\n",
    "# z = x+y # Element-wise addition\n",
    "# z = torch.add(x,y) # Same result\n",
    "# y.add_(x) # Inplace element-wise addition; functions if \"_\" before its name make inplace ops\n",
    "\n",
    "# z = x-y # Element-wise subtraction\n",
    "# z = torch.sub(x, y) # Analogous as above\n",
    "# y.sub_(x) # Analogous as above\n",
    "\n",
    "# z = x*y # Element-wise multiplication\n",
    "# z = torch.mul(x, y) # Analogous as above\n",
    "# y.mul_(x) # Analogous as above\n",
    "\n",
    "# z = x / y # Element-wise division\n",
    "# z = torch.div(x, y) # Analogous as above\n",
    "# y.div_(x) # Analogous as above\n",
    "\n",
    "### Slicing\n",
    "# z = x[:, 1]\n",
    "# z = x[1, 1]\n",
    "\n",
    "### Reshaping\n",
    "# z = x.view(4)\n",
    "# z = x.view(-1, 2)\n",
    "\n",
    "### Casting to numpy arrays\n",
    "# z = x.numpy()\n",
    "\n",
    "### Casting from numpy arrays to pytorch tensors\n",
    "# z = np.array([[1, 3.4], [5, 8.9]])\n",
    "# w = torch.from_numpy(z)\n",
    "\n",
    "### Working on GPU\n",
    "# if torch.cuda_is_available():\n",
    "#     gpu = torch.device(\"cuda\")\n",
    "#     x = torch.ones(1, 2, device=gpu) # Tensor allocated on GPU\n",
    "#     y = torch.ones(5) # Tensor allocated on CPU\n",
    "#     y = y.to(gpu) # Sending tensor instantiated on CPU to GPU\n",
    "\n",
    "### Observation\n",
    "#################################################################################\n",
    "#    When dealing with tensors that it'll be needed to compute its gradients    #\n",
    "#    (e.g in optimization), Pytorch requires defining it explicitly.            #\n",
    "#################################################################################\n",
    "# x = torch.ones(4, requires_grad=True) # requires_grad argument is False by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd's package: used to ease the calculation of gradients. Its principle is based on Automatic Differentiation (autodiff)\n",
    "which is a general method for automatically computing the derivative of a value. For more details, I highly recommend checking out\n",
    "Prof. Roger Grosse lecture's slides¹ about the technique and the following medium post² about Pytorch's implementation of it.\n",
    "\n",
    "1: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf <br>\n",
    "2: https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create an input tensor\n",
    "x = torch.rand(3, requires_grad=True) # Remember that it's necessary to enable requires_grad property\n",
    "\n",
    "# After creating the input tensor, let's define\n",
    "# an operation over this tensor.\n",
    "y = 3*x+2 # Creates the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational graph created after operating over the tensor**\n",
    "\n",
    "<img src=\"./assets/images/compt_graph.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([0.1130, 0.4408, 0.1186], requires_grad=True)\n",
      "Y: tensor([2.3389, 3.3224, 2.3559], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"X:\", x)\n",
    "print(\"Y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, tensor X has requires_grad property set as **True** and tensor Y has the last operation over X (addition) as its gradient function. \n",
    "In the next example, Z is a tensor computed multiplying Y by itself and your grad. function is the multiplication op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.4703, 11.0381,  5.5501], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod_vec = torch.ones(3)\n",
    "z.backward(dot_prod_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod_vec = torch.ones(3)\n",
    "y.backward(dot_prod_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
